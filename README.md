# InsiderScraper

InsiderScraper is a single page web app desiged to show recent insider stock purhcase filings from the SEC Edgar website. The data is a rolling 30-day collection (any insider purhcases made more than 30 days ago are not included in the shown data). 

The intent is to show very recent insider purchases of common stock in a simplistic structure. In some cases, these are used to gauge company sentiment and potentially bullish trends. 

Note that the source code shown here won't fully represent a functioning application - as this was a personal project, I've retained the fully functioning version in a private repostiory due to the necessesity of private keys and connection strings. In a production environment I would not have created this to be a single page web application, however I was simply eager to gain experience in Angular and the single page functionalities it provided.

The remainder of this readme will describe the arcthiecture used to create this project. 

# Azure Services
To create the automation necessary to continually pull and trim data, I used a variety of Azure services. These included:

- Azure Automation
- Azure Function
- Azure VM
- Cosmos DB

A description of the purpose of each service are as follows. Note that I will describe the linear flow of interactions between the services anteceding this.

## Azure Automation
3 runbooks were used with Azure Automation. 

1. Start VMs - A scheduled task to boot up the Linux VM which does the data retrieval and hygiene.
2. Invoke Scraper - This establishes an SSH connection with the Linux VM, invokes the script that pulls data from the SEC Edgar webite, performs necessary hygiene (described further below), and uploads the new data to the Cosmos DB instance. This is invoked via webhook at the end of Start VMs.
3. Stop VMs - A function invoked via a webhook, and stops the Linux VM. It is also scheduled to automatically run 3 hours after Start VMs in case the webhook call fails. 


## Azure Function
A function which is triggered upon a blob storage change (in this case, logs generated by the Linux VM), which checks for any errors encountered by the script. Errors will result in a SMS notification. This also invokes the Stop VMs webhook upon completion.

## Azure VM
This Linux VM serves as the compute necesssary to pull data from the SEC Edgar website, trim the data to focus only on purchases, merge with existing data stored in the Comsos DB instance, remove old data, and upload the updated and cleaned data back to the Cosmos DB instance.

## Cosmos DB
This stores the insider purchase transaction data used by the website.

#Flow of Azure Services
Below is a description of how data is gathered and formatted to be used by the single page web application.

1. A scheduled task runs and starts the Azure VM. Once the VM is started, the task invokes the "InvokeScraper Function" task via webhook.
2. InvokeScraper Function creates an SSH connection with the Linux VM and invokes the InsiderScraper python scrip. This script pulls data from the SEC website and trims it to only include the purchase data. Then, it pulls the data from the Cosmos DB and merges all data by Symbol (the company whose stock was purchased by an insider). Any purchases older than 30 days are also removed. Any errors encountered are logged in a log file. At the end, the log file is uploaded to blob storage and the purchase data is uploaded to Cosmos DB.
3. An Azure function is triggered upon the log file upload to blob storage. This parses the file for any errors generated that would have prevented a Cosmos DB data upload, and raises an error. This error results in an SMS notification so someone can investigate. Finally, this function invokes "Stop VMs" via a webook to turn off the Linux VM. The Stop VMs function is also a scheduled task in case this webhook invocation fails for any reason.

Once this flow is complete, the fresh SEC data is available for the web application to use. Below is the general layout and functionality of the website.

# The Web Application

The web application includes a header for navigation - the data itself is accessed via the "Insider Buys" dropdown, which includes different formats of organization. One is organized by symbol (the company whose shares were purhcased) and the other is a raw transaction table. Each transaction has its own row entry.

![image](https://user-images.githubusercontent.com/44855384/190000250-b0e17042-5e4e-4d3e-97a7-7200902669ba.png)

Below is a recording showing how one could use the Symbols page. Users can filter by symbol, the number of shares purhcases, number of buyers or purchases, and navigate through the data organized by symbol. Please note the below are just examples. There are other fields users can filter by, and users can leverage multiple filters at the same time.


Example of filtering by symbol:

https://user-images.githubusercontent.com/44855384/190001886-a54d932c-bde8-4f17-9f7f-c7c5817a6084.mp4


Example of digging through the symbol data:

https://user-images.githubusercontent.com/44855384/190002079-d279493a-58ea-40f1-bad4-ed7937174b8f.mp4



Using the navigation header at the top, users can also view raw transaction data. This can be filtered by most of the headers available in the table. Note that this page also has its own filter section which can be leveraged in the same manner as the previous examples provided for the Symbols page. An example is included below:


https://user-images.githubusercontent.com/44855384/190002716-30d4031f-bf0f-4d25-939b-2608907933fb.mp4







