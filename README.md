# InsiderScraper

Insider scraper is a single page web app desiged to show recent insider stock purhcase filings from the SEC Edgar website. The data is a rolling 30-day collection (any insider purhcases made more than 30 days ago are not included in the shown data). 

The intent is to show, in a simplistic structure, very recent insider purchases of common stock which in some cases are used to gauge company sentiment and potentially bullish trends. 

Note that the source code shown here won't fully represent a functioning application - as this was a personal project, I've retained the fully functioning version in a private repostiory due to the necessesity of private keys and connection strings. In a production environment I would not have created this to be a single page web application, however I simply was eager to experience angular and the single page functionalities it provided.

The remainder of this readme will describe the arcthiecture used to create this project. 

# Azure Services
To create the automation necessary to continually pull and trim data, I used a variety of Azure services. These included:

- Azure Automation
- Azure Function
- Azure VM
- Cosmos DB

A description of the purpose of each service are as follows. Note I will descrube the linear flow of interactions between the services after the description.

## Azure Automation
3 runbooks were used with Azure Automation. 

1. Start VMs - a scheduled task to simply boot up the Linux VM which does the data retrieval and hygeiene tassk
2. Invoke Scraper - this would establish an SSH connection with the linux VM and invoke the script which pulls data from the SEC Edgar webite, performs necessary hygiene (described further below), and uploads the new data to the Cosmos DB instance. THis is a webook and invoked at the end of Start VMs.
3. Stop VMs - a webhook which when invoked stoped the linux VM. It is also scheduled to automatically run 3 hours after Start VMs just in case. 


## Azure Function
A function which is triggered upon a blob storage change (in this dase, logs generated by the linux VM), which checks for any errors encountered by the script. Errors will results in a SMS notification. This also invokes the Stop VMs webhook upon completion.

## Azure VM
This Linux VM serves as the compute necesssary to pull data from the SEC Edgar website, trim the data to focus only on purchases, merge with existing data stored in the Comsos DB instance, remove old data, and  upload the updated and cleaned data back to the Cosmos DB instance.

## Cosmos DB
This stores the insider purchase transaction data used by the website.

#Flow of Azure Services
Below is a description of how, from beginning to end, data is gathered and formatted to be used by the single page web application.

1. A scheduled task runs and starts the Azure VM. Once the VM is started, the task invokes the "Invoke Scraper Function" task via webhook.
2. InvokeScraperFunction creates an SSH connection with the Linux VM and invokes the InsiderScraper python scrip. This script pulls data from the SEC website and trims it to only include the purchase data. Then, it pulls the data from the cosmos DB and merges all data by Symbol (the company whose stock was purchased by an insider). Any purchases older than 30 days are also removed. Any errors encountered are logged in a log file. At the end, the log file is uploaded to blob storage and the purchase data is uploaded to Cosmos DB.
3. An Azure function is triggered upon the log file upload to blob storage. This parses the file for any errors generated that would have prevented Cosmos DB data upload, and raises an error. This error results in a SMS notification so someone can investigate. Finally, this funciton invokes "Stop VMs" via a webook to turn off the Linux VM. Note, the Stop VMs function is also a scheduled task in case this webhook invocation fails for any reason.

Once this flow is complete, the fresh SEC data is available for the web application to use. Below is the general layout and functionality of the website.

# The Web Application

The web application includes a header for navigation - the data itself is accessed via the "Insider Buys" dropdown, which includes different formats of organization. One is organized by symbol (the company whose shares were purhcased) and the other is raw transaction table, each of which transation has its own row entry.

![image](https://user-images.githubusercontent.com/44855384/190000250-b0e17042-5e4e-4d3e-97a7-7200902669ba.png)

Below is a recording showing how one could use the Symbols page. Users can filter by symbol, the number of shares purhcases, number of buyers or purchases, and navigating through the data organized by symbol. Please note the below are just exmples. There are other fields users can filter by, and users can leverage multiple filters at the same time.


Here's an example of filtering by symbol:

https://user-images.githubusercontent.com/44855384/190001886-a54d932c-bde8-4f17-9f7f-c7c5817a6084.mp4


And another showing how you would dig through the symbol data:

https://user-images.githubusercontent.com/44855384/190002079-d279493a-58ea-40f1-bad4-ed7937174b8f.mp4



Using the navgation header at the top, users can also view raw transaction data. This can be filtered by most of the heaers available in the table. Note that this page also has its own filter section which can be leveraged in the same manner as the previous exmaples provided for the Symbols page. An example is included below:


https://user-images.githubusercontent.com/44855384/190002716-30d4031f-bf0f-4d25-939b-2608907933fb.mp4







